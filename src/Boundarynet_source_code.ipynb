{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad9b681e-370a-4cfa-a452-dd2d7f0cd77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.ops\n",
    "import torch_optimizer as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "exe_file = \"/mnt/ssd1/nyh/AI_Competition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3afb5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf9ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_img_shape = (1024, 1024)\n",
    "train_img_shape = (256, 256)\n",
    "train_resize_img_shape = (224, 224)\n",
    "valid_img_shape = (224, 224)\n",
    "test_img_shape = (224, 224)\n",
    "batch_size = 64\n",
    "threshold = 0.6\n",
    "random_seed = 20230705\n",
    "loss_func = [[\"DICE\",0.8], [\"FOCAL\",0.2]] # first = DICE, second = FOCAL\n",
    "model = smp.UnetPlusPlus\n",
    "\n",
    "model_name = [\"se_resnext101_32x4d\",\"imagenet\"]\n",
    "optimizer_name = \"RAdam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "812f64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seed_func(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5af0c197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 cpu cores\n",
      "Let's use 4 GPUs!\n"
     ]
    }
   ],
   "source": [
    "cpu_core_num = os.cpu_count()\n",
    "gpu_num = torch.cuda.device_count()\n",
    "\n",
    "print(f\"{cpu_core_num} cpu cores\")\n",
    "print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20ff3de5-0d0e-497b-ac75-d5179a3f65d3",
   "metadata": {},
   "source": [
    "## Utils\n",
    "- https://dacon.io/competitions/official/236092/talkboard/408732?page=1&dtype=recent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99aaf18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_score(prediction: np.array, ground_truth: np.array, smooth=1e-7):\n",
    "    '''\n",
    "    Calculate Dice Score between two binary masks.\n",
    "    '''\n",
    "    scores = []\n",
    "    for i in range(prediction.shape[0]):\n",
    "        intersection = np.sum(prediction[i] * ground_truth[i])\n",
    "        score = (2.0 * intersection + smooth) / (np.sum(prediction[i]) + np.sum(ground_truth[i]) + smooth)\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    scores = sum(scores)/prediction.shape[0]\n",
    "    return scores\n",
    "\n",
    "\n",
    "def calculate_dice_scores(ground_truth_df, prediction_df, img_shape=(224, 224)):\n",
    "    print(ground_truth_df.shape, prediction_df.shape)\n",
    "\n",
    "    # Find the indices where the prediction ids exist in the ground truth ids\n",
    "    prediction_ids = prediction_df.tolist()\n",
    "    ground_truth_ids = ground_truth_df.tolist()\n",
    "    matching_indices = [i for i in range(len(prediction_ids)) if prediction_ids[i] in ground_truth_ids]\n",
    "\n",
    "    # Filter the prediction and ground truth dataframes based on the matching indices\n",
    "    prediction_df = prediction_df[matching_indices]\n",
    "    ground_truth_df = ground_truth_df[matching_indices]\n",
    "\n",
    "    # # Extract the mask_rle columns\n",
    "    print(ground_truth_df.shape, prediction_df.shape)\n",
    "    pred_mask_rle = prediction_df\n",
    "    gt_mask_rle = ground_truth_df\n",
    "    # pred_mask_rle = prediction_df[:, 0, :, :].squeeze(0)\n",
    "    # gt_mask_rle = ground_truth_df.squeeze(0)\n",
    "\n",
    "    def calculate_dice(pred_rle, gt_rle):\n",
    "        pred_mask = rle_decode(pred_rle, img_shape)\n",
    "        gt_mask = rle_decode(gt_rle, img_shape)\n",
    "\n",
    "        if np.sum(gt_mask) > 0 or np.sum(pred_mask) > 0:\n",
    "            return dice_score(pred_mask, gt_mask)\n",
    "        else:\n",
    "            return None  # No valid masks found, return None\n",
    "\n",
    "    dice_scores = []\n",
    "    for pred_rle, gt_rle in zip(pred_mask_rle, gt_mask_rle):\n",
    "        dice_score = calculate_dice(pred_rle, gt_rle)\n",
    "        # print(dice_score)\n",
    "        if dice_score is not None:\n",
    "            dice_scores.append(dice_score)\n",
    "\n",
    "    return np.mean(dice_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "838e1d83-8670-407b-82f6-bf9652f58639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE 디코딩 함수\n",
    "\n",
    "def rle_decode(mask_rle, shape):\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    \n",
    "    return img.reshape(shape)\n",
    "\n",
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be76a29e-e9c2-411a-a569-04166f074184",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8496767-2f64-4285-bec4-c6f53a1fd9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, csv_file, mode = None, transform=None, infer=False):\n",
    "        \n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.mode = mode # index number // Train_test_split\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "        if mode is not None:\n",
    "            self.data = self.data.loc[self.mode]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        img_path = self.data.iloc[idx, 1]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "\n",
    "        mask_rle = self.data.iloc[idx, 2]\n",
    "        mask = cv2.imread(mask_rle)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc955893-22fd-4320-88be-7aa0d790cbd9",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f035f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "    [   \n",
    "        A.Resize(train_img_shape[0], train_img_shape[1]),\n",
    "        A.RandomCrop(train_resize_img_shape[0], train_resize_img_shape[1]),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p = 0.5),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "vaild_transform = A.Compose(\n",
    "    [   \n",
    "        A.Resize(valid_img_shape[0], valid_img_shape[1]),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transform = A.Compose(\n",
    "    [   \n",
    "        A.Resize(test_img_shape[0], test_img_shape[1]),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b708503-2ff9-4584-9d73-40990b3572f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/mnt/ssd1/nyh/AI_Competition/train_csv.csv\"\n",
    "len_data = len(pd.read_csv(img_path))\n",
    "dataset_list = [i for i in range(len_data)]\n",
    "random.shuffle(dataset_list)\n",
    "train_dataset_list = dataset_list[:int(len_data*0.9)]\n",
    "valid_dataset_list = dataset_list[int(len_data*0.9):]\n",
    "\n",
    "train_dataset = SatelliteDataset(csv_file=img_path , mode = train_dataset_list, transform=train_transform)\n",
    "valid_dataset = SatelliteDataset(csv_file=img_path, mode = valid_dataset_list, transform=vaild_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader.__len__(), valid_dataloader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86e57d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.UnetPlusPlus\n",
    "model = model(classes=2,\n",
    "                encoder_name=model_name[0],\n",
    "                encoder_weights=model_name[1]\n",
    "               ).to(device) # DeepLabV3Plus -> UnetPlutPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eb7714",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./best_se_resnext101_32x4d_val_87.14_(81.8).pt\")['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3af11aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub Boundary network.\n",
    "\n",
    "model2 = smp.UnetPlusPlus\n",
    "model2 = model2(classes=2,\n",
    "                in_channels = 4,\n",
    "                encoder_name=\"se_resnext50_32x4d\",\n",
    "                encoder_weights=model_name[1]\n",
    "               ).to(device) # DeepLabV3Plus -> UnetPlutPlus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0895765-fba0-4fd9-b955-a6c0e43012e9",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dc01695",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch\n",
    "ALPHA = 0.5\n",
    "BETA = 0.5\n",
    "\n",
    "class TverskyLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(TverskyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA):\n",
    "        \n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        TP = (inputs * targets).sum()    \n",
    "        FP = ((1-targets) * inputs).sum()\n",
    "        FN = (targets * (1-inputs)).sum()\n",
    "       \n",
    "        Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n",
    "        \n",
    "        return 1 - Tversky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e9687ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.5\n",
    "BETA = 0.5\n",
    "GAMMA = 1\n",
    "\n",
    "class FocalTverskyLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(FocalTverskyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA, gamma=GAMMA):\n",
    "        \n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        TP = (inputs * targets).sum()    \n",
    "        FP = ((1-targets) * inputs).sum()\n",
    "        FN = (targets * (1-inputs)).sum()\n",
    "        \n",
    "        Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n",
    "        FocalTversky = (1 - Tversky)**gamma\n",
    "                       \n",
    "        return FocalTversky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "661cc826",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=[0.25, 0.75], size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = torch.tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha[1] * (1 - pt) ** self.gamma * BCE_loss\n",
    "        if self.size_average:\n",
    "            return F_loss.mean()\n",
    "        else:\n",
    "            return F_loss.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e0c0de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  model = nn.DataParallel(model).to(device)\n",
    "\n",
    "def criterion(pred, ground_truth, loss_func):\n",
    "    bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    dice_loss = smp.losses.DiceLoss(mode='binary')\n",
    "    tversky_loss = TverskyLoss()\n",
    "    focal_loss = FocalLoss()\n",
    "    \n",
    "    bce_values = bce_loss(pred, ground_truth)\n",
    "    dice_value = dice_loss(pred, ground_truth)\n",
    "    tversky_value = tversky_loss(pred, ground_truth)\n",
    "    focal_values = focal_loss(pred, ground_truth)\n",
    "    \n",
    "    if loss_func[0][0] == \"DICE\": loss_value_1 = dice_value * loss_func[0][1]\n",
    "    elif loss_func[0][0] == \"BCE\": loss_value_1 = bce_values * loss_func[0][1] #BCE\n",
    "    elif loss_func[0][0] == \"TVERSKY\": loss_value_1 = tversky_value * loss_func[0][1] #tversky\n",
    "\n",
    "    if loss_func[1][0] == \"FOCAL\": loss_value_2 = focal_values * loss_func[1][1]\n",
    "\n",
    "    return loss_value_1 + loss_value_2\n",
    "\n",
    "if optimizer_name == \"RAdam\": optimizer = optim.RAdam(model.parameters(), lr=0.001)\n",
    "elif optimizer_name == \"adam\": optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer = optimizer, T_max=5) #learning_Rate를 조절하는 스케줄려를 설정, T_max는 반주기의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1f66ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 8/1607 [00:30<1:17:59,  2.93s/batch, dice=0.883, dice2=0.437, dice3=0.239, loss=0.588]"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "stop_iter = 15\n",
    "img_crop =  64\n",
    "\n",
    "optimizer = optim.RAdam(model2.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer = optimizer, T_max=10)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train_total_loss = 0.0\n",
    "    train_total_count = 0\n",
    "    train_total_count2 = 0.0\n",
    "    train_total_count3 = 0.0\n",
    "    train_total_data = 0\n",
    "\n",
    "    big_dice_score = 0.0\n",
    "    small_dice_score = 0.0\n",
    "    crop_dice_score = 0.0\n",
    "\n",
    "    val_total_loss = 0.0\n",
    "    val_total_count = 0\n",
    "    val_total_data = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    model2.train()\n",
    "    \n",
    "    with tqdm(train_dataloader,desc='Train',unit='batch') as tepoch:\n",
    "        for index, (images, masks) in enumerate(tepoch):\n",
    "\n",
    "            images, masks = images.float().to(device), masks.float().to(device)\n",
    "            \n",
    "            outputs = model(images).mean(axis = 1)\n",
    "            outputs_mask = torch.sigmoid(outputs).cpu().detach().numpy()\n",
    "            outputs_mask = (outputs_mask > threshold).astype(np.uint8)\n",
    "            dice = dice_score(outputs_mask, masks.cpu().detach().numpy())\n",
    "\n",
    "            crop_32_input, crop_32_gt, crop_32_gt_info = [], [], []\n",
    "            for i, img in enumerate(outputs_mask):\n",
    "                # indices = torch.where(torch.Tensor(img) == 1) # [Row, Col]\n",
    "                dst = cv2.dilate(img, cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))) # 모폴로지 팽창 연산\n",
    "                diff_clamped = torch.clamp(torch.Tensor(dst)- torch.Tensor(img), min=0, max=1) # 팽창 - 원본 => 경계\n",
    "                indices = torch.where(diff_clamped == 1) # [Row, Col]\n",
    "                xx, yy = torch.clamp(indices[0] - img_crop//2, min=0, max=224-img_crop), torch.clamp(indices[1] - img_crop//2, min=0, max=224-img_crop)\n",
    "\n",
    "                if xx.shape == torch.Tensor([]).shape: continue\n",
    "                boundary_mask = [torch.stack([x, y, x+img_crop, y+img_crop]) for x, y in zip(xx, yy)]\n",
    "                boundary_mask = torch.stack(boundary_mask)\n",
    "                boundary_mask_output = torchvision.ops.nms(boundary_mask[:, :4].float(), torch.ones(boundary_mask.shape[0]), 0.3)\n",
    "\n",
    "                # bn_idx = boundary_mask[boundary_mask_output]\n",
    "                for x, y, _, _ in  boundary_mask[boundary_mask_output]:\n",
    "                    crop_32_input.append(torch.cat([images[i, :, x:x+img_crop, y:y+img_crop], torch.Tensor(outputs_mask[i, x:x+img_crop, y:y+img_crop].reshape(1, img_crop, img_crop)).to(device)], axis = 0)) # 입력 이미지 차원 [(B+num), 4, 32, 32]\n",
    "                    crop_32_gt.append(masks[i, x:x+img_crop, y:y+img_crop]) # 정답 차원 [(B+num), 32,32]\n",
    "                    crop_32_gt_info.append([i,x,y])\n",
    "                \n",
    "            if crop_32_input.__len__() == 0: continue\n",
    "            crop_32_input = torch.stack(crop_32_input)\n",
    "            crop_32_gt = torch.stack(crop_32_gt)\n",
    "\n",
    "            crop_32_input, crop_32_gt = crop_32_input.to(device), crop_32_gt.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model2(crop_32_input).mean(1)\n",
    "            pred_32_mask = torch.sigmoid(pred).cpu().detach().numpy()\n",
    "            pred_32_mask = (pred_32_mask > threshold).astype(np.uint8)\n",
    "\n",
    "            for (batch_num_idx, idx_x, idx_y), pred_32_mask_one in zip(crop_32_gt_info, pred_32_mask):\n",
    "                outputs_mask[batch_num_idx, idx_x:idx_x+img_crop, idx_y:idx_y+img_crop] = pred_32_mask_one\n",
    "\n",
    "            dice2 = dice_score(outputs_mask, masks.cpu().detach().numpy())        \n",
    "            dice3 = dice_score(pred_32_mask, crop_32_gt.cpu().detach().numpy())        \n",
    "            loss = criterion(pred, crop_32_gt, loss_func)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_total_count += dice\n",
    "            train_total_count2 += dice2\n",
    "            train_total_count3 += dice3\n",
    "            train_total_loss += loss.item()\n",
    "            tepoch.set_postfix(loss=train_total_loss/(index+1), dice = train_total_count/(index+1), dice2 = train_total_count2/(index+1), dice3 = train_total_count3/(index+1))\n",
    "            \n",
    "    train_total_loss /= index\n",
    "    train_accuracy = train_total_count / index * 100\n",
    "    print(f\"Train Loss: {train_total_loss:.4f}\\t\\tTrain acc: {train_accuracy} Train_acc2 {train_total_count2/index * 100}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    model2.eval()\n",
    "    val_total_count2 = 0.0\n",
    "    val_total_count3 = 0.0\n",
    "    with tqdm(valid_dataloader,desc='Validation',unit='batch') as tepoch:\n",
    "        for index, (images, masks) in enumerate(tepoch):\n",
    "            images = images.float().to(device)\n",
    "            masks = masks.float().to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                outputs = model(images).mean(axis = 1)\n",
    "                outputs_mask = torch.sigmoid(outputs).cpu().detach().numpy()\n",
    "                outputs_mask = (outputs_mask > threshold).astype(np.uint8) # Threshold = 0.35\n",
    "                dice = dice_score(outputs_mask, masks.cpu().detach().numpy())\n",
    "\n",
    "                crop_32_input, crop_32_gt, crop_32_gt_info = [], [], []\n",
    "                for i, img in enumerate(outputs_mask):\n",
    "                    dst = cv2.dilate(img, cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))) # 모폴로지 팽창 연산\n",
    "                    diff_clamped = torch.clamp(torch.Tensor(dst)- torch.Tensor(img), min=0, max=1) # 팽창 - 원본 => 경계\n",
    "                    indices = torch.where(diff_clamped == 1) # [Row, Col]\n",
    "                    # indices = torch.where(torch.Tensor(img) == 1) # [Row, Col]\n",
    "                    xx, yy = torch.clamp(indices[0] - img_crop//2, min=0, max=224-img_crop), torch.clamp(indices[1] - img_crop//2, min=0, max=224-img_crop)\n",
    "\n",
    "                    if xx.shape == torch.Tensor([]).shape: continue\n",
    "                    boundary_mask = [torch.stack([x, y, x+img_crop, y+img_crop]) for x, y in zip(xx, yy)]\n",
    "                    boundary_mask = torch.stack(boundary_mask)\n",
    "                    boundary_mask_output = torchvision.ops.nms(boundary_mask[:, :4].float(), torch.ones(boundary_mask.shape[0]), 0.3)\n",
    "\n",
    "                    # bn_idx = boundary_mask[boundary_mask_output]\n",
    "                    for x, y, _, _ in  boundary_mask[boundary_mask_output]:\n",
    "                        crop_32_input.append(torch.cat([images[i, :, x:x+img_crop, y:y+img_crop], torch.Tensor(outputs_mask[i, x:x+img_crop, y:y+img_crop].reshape(1, img_crop, img_crop)).to(device)], axis = 0)) # 입력 이미지 차원 [(B+num), 4, 32, 32]\n",
    "                        crop_32_gt.append(masks[i, x:x+img_crop, y:y+img_crop]) # 정답 차원 [(B+num), 32,32]\n",
    "                        crop_32_gt_info.append([i,x,y])\n",
    "                \n",
    "                if crop_32_input.__len__() == 0: continue\n",
    "                crop_32_input = torch.stack(crop_32_input)\n",
    "                crop_32_gt = torch.stack(crop_32_gt)\n",
    "\n",
    "                crop_32_input, crop_32_gt = crop_32_input.to(device), crop_32_gt.to(device)\n",
    "            \n",
    "            if crop_32_input.__len__() == 0: continue\n",
    "            pred = model2(crop_32_input).mean(1)\n",
    "            pred_32_mask = torch.sigmoid(pred).cpu().detach().numpy()\n",
    "            pred_32_mask = (pred_32_mask > threshold).astype(np.uint8)\n",
    "\n",
    "            for (batch_num_idx, idx_x, idx_y), pred_32_mask_one in zip(crop_32_gt_info, pred_32_mask):\n",
    "                    outputs_mask[batch_num_idx, idx_x:idx_x+img_crop, idx_y:idx_y+img_crop] = pred_32_mask_one\n",
    "\n",
    "            dice2 = dice_score(outputs_mask, masks.cpu().detach().numpy())\n",
    "            dice3 = dice_score(pred_32_mask, crop_32_gt.cpu().detach().numpy())\n",
    "            # dice2 = dice_score(pred_32_mask, crop_32_gt.cpu().detach().numpy())        \n",
    "            loss = criterion(pred, crop_32_gt, loss_func)\n",
    "\n",
    "\n",
    "            val_total_loss += loss.item()\n",
    "            val_total_count += dice\n",
    "            val_total_count2 += dice2\n",
    "            val_total_count3 += dice3\n",
    "\n",
    "            tepoch.set_postfix(loss=val_total_loss/(index+1), dice = val_total_count/(index+1), dice2 = val_total_count2/(index+1), dice3 = val_total_count3/(index+1))\n",
    "            # tepoch.set_postfix(loss=val_total_loss/(index+1))\n",
    "\n",
    "    val_total_loss /= index\n",
    "    val_accuracy = val_total_count / index * 100\n",
    "    print(f\"Valid Loss: {val_total_loss:.4f}\\t\\tValid acc: {val_accuracy}, Vac_2 {val_total_count2/index * 100}\")\n",
    "    print(f\"Loss: {val_total_loss:.4f}\")\n",
    "    val_accuracy = (val_total_count2/index * 100) - val_accuracy\n",
    "\n",
    "    scheduler.step(epoch)\n",
    "\n",
    "    if epoch == 0: \n",
    "        best_acc = val_accuracy\n",
    "        save_file = \"best.pt\"\n",
    "        torch.save({'model_state_dict':model2.module.state_dict() if torch.cuda.device_count() > 1 else model.state_dict(), 'scheduler':scheduler}, save_file)\n",
    "        stop_count = 0\n",
    "    else:\n",
    "        if best_acc < val_accuracy:\n",
    "            best_acc = val_accuracy\n",
    "            save_file = \"best.pt\"\n",
    "            stop_count = 0\n",
    "            torch.save({'model_state_dict':model2.module.state_dict() if torch.cuda.device_count() > 1 else model.state_dict(), 'scheduler':scheduler}, save_file)\n",
    "            print(f\"IMPROVE ACCURACY {val_accuracy:.4f}\")\n",
    "        else:\n",
    "            print(f\"{val_accuracy}, FAIL\")\n",
    "            stop_count += 1\n",
    "    \n",
    "    \n",
    "    if stop_count > stop_iter:\n",
    "        print('Early Stopping')\n",
    "        break\n",
    "    \n",
    "    print('=' * 30, epoch)\n",
    "\n",
    "print('=' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d037dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/mnt/ssd1/nyh/AI_Competition/test.csv\"\n",
    "test_dataset = SatelliteDataset(csv_file=img_path, mode = None, transform=test_transform, infer=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8600a3cd",
   "metadata": {},
   "source": [
    "## 경계 개선 네트워크 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d936160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/474 [00:08<1:05:24,  8.30s/it]/home/eslab/anaconda3/envs/pytorch_medical/lib/python3.7/site-packages/ipykernel_launcher.py:66: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "/home/eslab/anaconda3/envs/pytorch_medical/lib/python3.7/site-packages/ipykernel_launcher.py:72: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "100%|██████████| 474/474 [11:25<00:00,  1.45s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = smp.UnetPlusPlus(classes=2,\n",
    "                encoder_name=\"se_resnext101_32x4d\",\n",
    "                encoder_weights=\"imagenet\"\n",
    "               ).to(device)\n",
    "\n",
    "model2 = smp.UnetPlusPlus(classes=2,\n",
    "                encoder_name=\"se_resnext50_32x4d\",\n",
    "                encoder_weights=\"imagenet\",\n",
    "                in_channels=4\n",
    "               ).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"./best_se_resnext101_32x4d_val_87.14_(81.8).pt\")['model_state_dict']) # 경계를 개선하고자 하는 모델 파일 경로\n",
    "model2.load_state_dict(torch.load(\"./best_boundry_plus_0.14.pt\")['model_state_dict'])  # 경계 개선 네트워크 경로\n",
    "model = nn.DataParallel(model).to(device) \n",
    "model2 = nn.DataParallel(model2).to(device)\n",
    "\n",
    "img_crop = 64\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    result = []\n",
    "    for images in tqdm(test_dataloader):\n",
    "        images = images.float().to(device)\n",
    "\n",
    "        outputs = model(images).mean(axis = 1).unsqueeze(1)\n",
    "        masks = torch.sigmoid(outputs).cpu().numpy()\n",
    "        masks = np.squeeze(masks, axis=1)\n",
    "        masks = (masks > threshold).astype(np.uint8) # Threshold = 0.35\n",
    "\n",
    "        crop_32_input, crop_32_gt, crop_32_gt_info = [], [], []\n",
    "        for i, img in enumerate(masks):\n",
    "            dst = cv2.dilate(img, cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))) # 모폴로지 팽창 연산\n",
    "            diff_clamped = torch.clamp(torch.Tensor(dst)- torch.Tensor(img), min=0, max=1) # 팽창 - 원본 => 경계\n",
    "            indices = torch.where(diff_clamped == 1) # [Row, Col]\n",
    "            xx, yy = torch.clamp(indices[0] - img_crop//2, min=0, max=224-img_crop), torch.clamp(indices[1] - img_crop//2, min=0, max=224-img_crop)\n",
    "\n",
    "            if xx.shape == torch.Tensor([]).shape: continue\n",
    "            boundary_mask = [torch.stack([x, y, x+img_crop, y+img_crop]) for x, y in zip(xx, yy)]\n",
    "            boundary_mask = torch.stack(boundary_mask)\n",
    "            boundary_mask_output = torchvision.ops.nms(boundary_mask[:, :4].float(), torch.ones(boundary_mask.shape[0]), 0.3)\n",
    "\n",
    "            for x, y, _, _ in  boundary_mask[boundary_mask_output]:\n",
    "                crop_32_input.append(torch.cat([images[i, :, x:x+img_crop, y:y+img_crop], torch.Tensor(masks[i, x:x+img_crop, y:y+img_crop].reshape(1, img_crop, img_crop)).to(device)], axis = 0)) # 입력 이미지 차원 [(B+num), 4, 32, 32]\n",
    "                crop_32_gt_info.append([i,x,y])\n",
    "\n",
    "        if crop_32_input.__len__() == 0: continue\n",
    "        crop_32_input = torch.stack(crop_32_input)\n",
    "        crop_32_input = crop_32_input.to(device)\n",
    "    \n",
    "        pred = model2(crop_32_input).mean(1)\n",
    "        pred_32_mask = torch.sigmoid(pred).cpu().detach().numpy()\n",
    "        pred_32_mask = (pred_32_mask > threshold).astype(np.uint8)\n",
    "\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(masks[0])\n",
    "\n",
    "        for (batch_num_idx, idx_x, idx_y), pred_32_mask_one in zip(crop_32_gt_info, pred_32_mask):\n",
    "            masks[batch_num_idx, idx_x:idx_x+img_crop, idx_y:idx_y+img_crop] = pred_32_mask_one\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(masks[0])\n",
    "        \n",
    "        \n",
    "        for i in range(len(images)):\n",
    "            mask_rle = rle_encode(masks[i])\n",
    "            if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n",
    "                result.append(-1)\n",
    "            else:\n",
    "                result.append(mask_rle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6543d00-32b3-4f2d-a572-d0879fd0a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('/mnt/ssd1/nyh/AI_Competition/sample_submission.csv')\n",
    "\n",
    "submit['mask_rle'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10cb6f-0826-4755-a376-97b695ae8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.14 (NGC 22.12/Python 3.8) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
